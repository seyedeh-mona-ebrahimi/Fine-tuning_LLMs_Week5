# Fine-tuning_LLMs_Week5


README: Fine-Tuning a Small Language Model

Project Overview

This project involves fine-tuning a small language model using the Hugging Face ecosystem. The model was trained on a selected dataset with optimizations for efficiency and performance. The notebook documents the training process, error resolution, and model evaluation.

Repository Contents

Notebook (.ipynb): Contains the full code for fine-tuning the model.

Error Fixes & Documentation: Detailed explanations of encountered errors and their resolutions.

Trained Model: The fine-tuned model, uploaded to Hugging Face.

Screenshots: Snapshots of the Hugging Face repository and evaluation results.

Project Steps

Cloned and executed the provided Jupyter Notebook.

Resolved errors encountered during execution.

Loaded and pre-processed the dataset.

Configured BitsAndBytes for quantization.

Fine-tuned a small Hugging Face model (LLaMA 1.1B or equivalent).

Evaluated the model qualitatively and quantitatively.

Saved and uploaded the trained model to Hugging Face.

Captured and documented results.

Submitted the final notebook with fixes and evaluations.

Model Training Configuration

Max Steps: 500

Logging Steps: 50

Evaluation Steps: 50

Quantization: Enabled via BitsAndBytes

Model & Results

Hugging Face Model: [Insert Model Link Here]

Evaluation Metrics: ROUGE and manual assessment.

Zero-Shot Inference Tests: Documented in the notebook.

Submission & References

The final notebook is available in this repository.

The Hugging Face model is publicly accessible.

The repository includes documentation of all encountered issues and solutions.

