# Fine-Tuning an LLM with LoRA & PEFT 


## 📌 Overview

This project fine-tunes a large language model (LLM) using LoRA (Low-Rank Adaptation) and PEFT (Parameter-Efficient Fine-Tuning). The goal was to enhance the model’s ability to generate better summaries while keeping the process efficient and lightweight. After fine-tuning, the model was evaluated and uploaded to Hugging Face for future use.


## 🔍 What Was Done?

**Set up the fine-tuning environment inside a Jupyter Notebook**

**Fine-tuned an instruction-following LLM using LoRA & PEFT**

**Fixed errors & optimized parameters to improve training stability**

**Evaluated model performance against original and human-written summaries**

**Uploaded the final fine-tuned model to Hugging Face**



## 🚀 How to Use the Fine-Tuned Model

Simply clone this repository, and run each cell one by one, feel free to adjust and change where you need to, and see the summaries!

## 📢 Key Learnings & Findings

LoRA & PEFT make fine-tuning more efficient without requiring massive GPU power.
Fine-tuning improved the model’s ability to generate more accurate and human-like summaries.
Debugging errors and tweaking hyperparameters were crucial for getting good results.

## 🔮 Next Steps

Further optimize the model to enhance coherence and consistency in summaries.
Experiment with different datasets to generalize performance.
Explore other fine-tuning methods to compare efficiency and output quality.
